---
title: "Regresión con datos de NFL_attendance"
author: "Basado en el blog de Julia Silge"
date: "29/9/2020"
output: 
  html_document:
     css: xaringan-themer.css
     dev: png
     highlight: "default"
     toc: true
     toc_float: true
     code_folding: hide
---



```{r librerias-utiles, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(hrbrthemes)
theme_set(theme_fivethirtyeight())

```



## Dataset

Utilizaremos los datos de [#Tidytuesday](https://github.com/rfordatascience/tidytuesday), la cual es una iniciativa de la comunidad de R, en la cual se publica semanalmente un set de datos con el objetivo de practicar las habilidades de procesamiento, visualización y modelado de datos. El dataset elegido contiene datos de la NFL, liga de fútbol americano de USA.

## Lectura de datos

```{r nfl_attandance, include=FALSE}
attendance <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/attendance.csv")
standings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/standings.csv")

```
```{r vista-dataset}
attendance %>% glimpse()
standings %>%  glimpse()

```


## Unión de los datasets

Ya leímos el dataset *attendance* y *standings*:

- attendance: datos de la asistencia semanal de los equipos. 

- standings: datos de la clasificación de los equipos con sus respectivas marcas. 

A continuación vamos a unir ambos datasets utilizando la sentencia **left_join** y mediante las siguientes variables: **c("year", "team_name", "team"))**



```{r union_datasets, echo=FALSE, message=FALSE, warning=FALSE}

attendance_joined <- attendance %>%
  left_join(standings,
    by = c("year", "team_name", "team")
  )

attendance_joined %>% view()
```


## Objetivo
Nuestro objetivo es obtener un modelo capaz predecir la asistencia semanal o 'weekly_attendance' a la NFL a partir del conjunto de datos #TidyTuesday. Hasta ahora, sabemos que es necesario utilizar modelos de regresión.


## Análisis exploratorio inicial

```{r boxplot-inicial, echo=FALSE, message=FALSE, warning=FALSE, fig.height=7, fig.width=10, fig.show = "hold", fig.align='center'}

attendance_joined %>%
  filter(!is.na(weekly_attendance)) %>%
  ggplot(aes(fct_reorder(team_name, weekly_attendance),
    weekly_attendance,
    fill = playoffs)) +
  geom_boxplot(outlier.alpha = 0.5) +
  coord_flip() +
  labs(
    fill = NULL, x = NULL,
    y = "Asistencia semanal de juegos de la NFL"
  )

# playoffs: eliminatorias, columna que contiene 2 posibles valores: eliminado y no eliminado.
```





Tengamos en cuenta que para los 32 equipos de la NFL, existen años en que lo dieron todo y aún así no llegaron a las eliminatorias o playoffs, lo que será bueno para modelar.

### ¿Cuánto influye el ´margin_of_victory´, una medida de los puntos anotados en relación con los puntos pérdidos, para llegar a los playoffs o eliminatorias?

```{r histograma, warning=FALSE, fig.height=7, fig.width=10, fig.show = "hold", fig.align='center'}
attendance_joined %>%
  distinct(team_name, year, margin_of_victory, playoffs) %>%
  ggplot(aes(margin_of_victory, fill = playoffs)) +
  geom_histogram(position = "identity", alpha = 0.7) +
  labs(
    x = "Margen de victoria",
    y = "Númmero de equipos",
    fill = NULL
  )


```


### ¿Hay cambios en cada semana de la temporada?

```{r boxplot, warning=FALSE, fig.height=5, fig.width=8, fig.show = "hold", fig.align='center'}
attendance_joined %>%
  mutate(week = factor(week)) %>%
  ggplot(aes(week, weekly_attendance, fill = week)) +
  geom_boxplot(show.legend = FALSE, outlier.alpha = 0.5) +
  labs(
    x = "Semana de la temporada de la NFL",
    y = "Asistencia semanal de juegos de la NFL"
  )
```

Hemos realizado un breve análisis exploratorio de datos, el cual siempre es una parte importante de la tarea de modelado. El siguiente paso es generar un conjunto de datos para modelar.


Eliminemos las semanas que cada equipo no jugó (es decir, donde la asistencia semanal es NA).
Conservemos únicamente las columnas que queremos usar para el modelado. Por ejemplo, mantendremos *margin_of_victory* y *Strength_of_schedule*, pero no *simple_rating*, que es la suma de esas dos primeras cantidades.


```{r elimino_na, message=FALSE, warning=FALSE}
attendance_df <- attendance_joined %>%
  filter(!is.na(weekly_attendance)) %>%
  select(
    weekly_attendance, team_name, year, week,
    margin_of_victory, strength_of_schedule, playoffs
  )
attendance_df
attendance_df_new <- attendance_df %>% drop_na()
attendance_df_new
```


## Modelos simples con tidymodels

¡Ahora es el momento de cargar el metapaquete tidymodels! 💪 El primer paso aquí es dividir nuestros datos en pruebas de entrenamiento y pruebas. Podemos usar **initial_split ()** para crear estos conjuntos de datos, divididos para que cada uno tenga aproximadamente la misma cantidad de ejemplos de equipos que pasaron a los playoffs.


```{r division_datos, message=FALSE, warning=FALSE}
library(tidymodels)  #rsample

set.seed(1234)
attendance_split <- attendance_df_new %>%
  initial_split(strata = playoffs)

nfl_train <- training(attendance_split)
nfl_test <- testing(attendance_split)

nfl_test
nfl_train
```

## Especificación del modelo **lm**

```{r regresion_lineal, message=FALSE, warning=FALSE}
lm_spec <- linear_reg() %>%   #parsnip
  set_engine(engine = "lm")

```

## Ajuste del modelo utilizando **lm**
```{r ajuste_lm, message=FALSE, warning=FALSE}
lm_fit <- lm_spec %>%
  fit(weekly_attendance ~ .,
    data = nfl_train)

lm_fit
```




Ya fiteamos el primer modelo, ahora vamos por el segundo.

```{r random_forest, message=FALSE, warning=FALSE }

rf_spec <- rand_forest(mode = "regression") %>%
  set_engine("ranger")

rf_spec

```


## Ajuste del modelo utilizando **random forest**

```{r}
library('ranger')
rf_fit <- rf_spec %>%
  fit(weekly_attendance ~ ., data = nfl_train)

rf_fit

```

Aclaración: hemos ajustado ambos modelos utilizando ´nfl_train´, es decir, los datos de entrenamiento. No hemos tocado los datos de las pruebas durante el entrenamiento.


## Evaluación de los modelos
Cuando sea el momento de evaluar nuestros modelos (para estimar qué tan bien funcionarán nuestros modelos con datos nuevos), analizaremos **nfl_test**. Podemos predecir cuál será la asistencia semanal a los partidos de la NFL tanto para los datos de entrenamiento como para los datos de prueba utilizando los modelos lm y de random forest. Uno de los objetivos de tidymodels es poder utilizar código como el siguiente de formas predecibles y coherentes para muchos tipos de modelos, y utilizar las herramientas de tidyverse adecuadas para este tipo de tareas.


```{r resultados, message=FALSE, warning=FALSE}
results_train <- lm_fit %>%
  predict(new_data = nfl_train) %>%
  mutate(
    truth = nfl_train$weekly_attendance,
    model = "lm"
  ) %>%
  bind_rows(rf_fit %>%
    predict(new_data = nfl_train) %>%
    mutate(
      truth = nfl_train$weekly_attendance,
      model = "rf"
    ))

results_test <- lm_fit %>%
  predict(new_data = nfl_test) %>%
  mutate(
    truth = nfl_test$weekly_attendance,
    model = "lm"
  ) %>%
  bind_rows(rf_fit %>%
    predict(new_data = nfl_test) %>%
    mutate(
      truth = nfl_test$weekly_attendance,
      model = "rf"
    ))
results_test
```

Para este modelo de regresión, la métrica que tendremos en cuenta para evaluar el modelo es el **rmse** o la raíz del error cuadrático medio de lo que hemos hecho hasta ahora.

```{r resultado_rsme_train, message=FALSE, warning=FALSE}
results_train %>%
  group_by(model) %>%
  rmse(truth = truth, estimate = .pred) %>% view

```

```{r resultado_rsme_test, message=FALSE, warning=FALSE}
results_test %>%
  group_by(model) %>%
  rmse(truth = truth, estimate = .pred) #%>% view
results_test
```


## ¿Malas decisiones?
Si miramos los datos de entrenamiento, el modelo de bosque aleatorio funcionó mucho mejor que el modelo lineal; el rmse es mucho menor. Sin embargo, no se puede decir lo mismo de los datos de prueba. 😭 La métrica para el entrenamiento y las pruebas para el modelo lineal es aproximadamente la misma, lo que significa que no hemos sobreajustado. Para el modelo de bosque aleatorio, la rmse es mucho más alta para los datos de prueba que para los datos de entrenamiento. Nuestros datos de entrenamiento no nos dan una buena idea de cómo funcionará nuestro modelo, y este poderoso algoritmo ML se ha sobreajustado a este conjunto de datos.


```{r grafico_resultado_test, echo=TRUE, fig.align='center', fig.height=5, fig.show="hold", fig.width=8, message=FALSE, warning=FALSE}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
    mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.5) +
  facet_wrap(~train) +
  labs(
    x = "Valor real",
    y = "Asitencia predicha",
    color = "Tipo de modelo"
  )

```



## Referencias:
 * [Blog de Julia Silge](https://juliasilge.com/blog/intro-tidymodels/)
 
 * [Sitio de Tidymodels](https://www.tidymodels.org/)
 
 * [Machine Learning con R y tidymodels](https://www.cienciadedatos.net/documentos/59_machine_learning_con_r_y_tidymodels#divisi%C3%B3n_train_y_test)

